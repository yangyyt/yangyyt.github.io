<!DOCTYPE html>



  


<html class="theme-next muse use-motion" lang="zh-CN">
<head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css">







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="大数据处理,">










<meta name="description" content="Spark简介参考1     参考2  Apache Spark 是一个新兴的大数据处理通用引擎，提供了分布式的内存抽象。Spark 拥有多种语言的函数式编程 API，提供了除 map 和 reduce 之外更多的运算符，这些操作是通过一个称作弹性分布式数据集(resilient distributed datasets, RDDs)的分布式数据框架进行的。   Spark 是一种与 Hadoop">
<meta name="keywords" content="大数据处理">
<meta property="og:type" content="article">
<meta property="og:title" content="Spark初步了解">
<meta property="og:url" content="http://yoursite.com/2019/03/24/Spark/index.html">
<meta property="og:site_name" content="YYT">
<meta property="og:description" content="Spark简介参考1     参考2  Apache Spark 是一个新兴的大数据处理通用引擎，提供了分布式的内存抽象。Spark 拥有多种语言的函数式编程 API，提供了除 map 和 reduce 之外更多的运算符，这些操作是通过一个称作弹性分布式数据集(resilient distributed datasets, RDDs)的分布式数据框架进行的。   Spark 是一种与 Hadoop">
<meta property="og:locale" content="zh-CN">
<meta property="og:image" content="http://yoursite.com/2019/03/24/Spark/D:/noSystem/yangyyt.github.io/source/_posts/Spark/Spark构成.png">
<meta property="og:image" content="http://yoursite.com/2019/03/24/Spark/D:/noSystem/yangyyt.github.io/source/_posts/Spark/Streaming图解.png">
<meta property="og:updated_time" content="2019-03-24T03:43:36.657Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Spark初步了解">
<meta name="twitter:description" content="Spark简介参考1     参考2  Apache Spark 是一个新兴的大数据处理通用引擎，提供了分布式的内存抽象。Spark 拥有多种语言的函数式编程 API，提供了除 map 和 reduce 之外更多的运算符，这些操作是通过一个称作弹性分布式数据集(resilient distributed datasets, RDDs)的分布式数据框架进行的。   Spark 是一种与 Hadoop">
<meta name="twitter:image" content="http://yoursite.com/2019/03/24/Spark/D:/noSystem/yangyyt.github.io/source/_posts/Spark/Spark构成.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Muse',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/2019/03/24/Spark/">





  <title>Spark初步了解 | YYT</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-CN">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">YYT</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br>
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br>
            
            Archives
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/03/24/Spark/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="joyes">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="YYT">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">Spark初步了解</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-03-24T10:18:56+08:00">
                2019-03-24
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <h4 id="Spark简介"><a href="#Spark简介" class="headerlink" title="Spark简介"></a>Spark简介</h4><p><a href="https://www.cnblogs.com/shiyanlou/p/6183389.html" target="_blank" rel="noopener">参考1</a>     <a href="https://www.shiyanlou.com/try/spark" target="_blank" rel="noopener">参考2</a></p>
<blockquote>
<p>Apache Spark 是一个新兴的大数据处理通用引擎，提供了分布式的内存抽象。Spark 拥有多种语言的函数式编程 API，提供了除 map 和 reduce 之外更多的运算符，这些操作是通过一个称作弹性分布式数据集(resilient distributed datasets, RDDs)的分布式数据框架进行的。</p>
</blockquote>
<blockquote>
<p>Spark 是一种与 Hadoop 相似的开源集群计算环境，但是两者之间还存在一些不同之处，这些有用的不同之处使 Spark 在某些工作负载方面表现得更加优越，换句话说，Spark 启用了内存分布数据集，除了能够提供交互式查询外，它还可以优化迭代工作负载。</p>
</blockquote>
<blockquote>
<p>Spark 是在 Scala 语言中实现的，它将 Scala 用作其应用程序框架。与 Hadoop 不同，Spark 和 Scala 能够紧密集成，其中的 Scala 可以像操作本地集合对象一样轻松地操作分布式数据集。</p>
</blockquote>
<a id="more"></a>
<h4 id="Spark生态圈"><a href="#Spark生态圈" class="headerlink" title="Spark生态圈"></a>Spark生态圈</h4><blockquote>
<p>Spark力图整合<strong>机器学习（MLib）</strong>、<strong>图算法（GraphX）</strong>、<strong>流式计算（Spark Streaming）</strong>和<strong>数据仓库（Spark SQL）</strong>等领域，通过计算引擎Spark，弹性分布式数据集（RDD），架构出一个新的大数据应用平台。</p>
</blockquote>
<blockquote>
<p>Spark生态圈以HDFS、S3、Techyon为底层存储引擎，以Yarn、Mesos和Standlone作为资源调度引擎；</p>
<p>使用Spark，可以实现MapReduce应用；</p>
<p>基于Spark，Spark SQL可以实现即时查询，Spark Streaming可以处理实时应用，MLib可以实现机器学习算法，GraphX可以实现图计算，SparkR可以实现复杂数学计算。</p>
</blockquote>
<p><img src="/2019/03/24/Spark/D:/noSystem\yangyyt.github.io\source\_posts\Spark\Spark构成.png" alt="Spark生态"></p>
<h4 id="Spark网上推荐学习路线"><a href="#Spark网上推荐学习路线" class="headerlink" title="Spark网上推荐学习路线"></a>Spark网上推荐学习路线</h4><blockquote>
<ul>
<li>Spark SQL<ul>
<li>分布式查询引擎</li>
</ul>
</li>
<li>Spark Streaming<ul>
<li>实时处理流式数据</li>
<li><img src="/2019/03/24/Spark/D:/noSystem\yangyyt.github.io\source\_posts\Spark\Streaming图解.png" alt="图解"></li>
</ul>
</li>
<li>Spark MLlib<ul>
<li>Spark中机器学习库算法</li>
</ul>
</li>
<li>Spark GraphX<ul>
<li>Graphx中包含简化图分析任务的图计算算法</li>
</ul>
</li>
<li>SparkR<ul>
<li>一个提供轻量级前端的R包</li>
<li>集成了Spark的分布式计算和存储等特性</li>
</ul>
</li>
<li>Spark DataFrame<ul>
<li>UDF</li>
</ul>
</li>
<li>Sqoop<ul>
<li>数据迁移工具</li>
<li>大数据环境中重要的是数据转换工具</li>
</ul>
</li>
<li>Spark大数据实验<ul>
<li>可以参考网上案例</li>
</ul>
</li>
</ul>
</blockquote>
<h4 id="实验楼Spark快速入门教程-参考链接"><a href="#实验楼Spark快速入门教程-参考链接" class="headerlink" title="实验楼Spark快速入门教程   参考链接"></a>实验楼Spark快速入门教程   <a href="https://www.shiyanlou.com/try/spark/trying" target="_blank" rel="noopener">参考链接</a></h4><p>（实验楼的环境似乎有误，并不能正常运行——–下来再参考别的资料进行学习）</p>
<p>分布式计算框架要解决两个问题：如何分发数据和如何分发计算。Hadoop 使用 HDFS 来解决分布式数据问题，MapReduce 计算范式提供有效的分布式计算。类似的，Spark 拥有多种语言的函数式编程 API，提供了除 map 和 reduce 之外更多的运算符，这些操作是通过一个称作弹性分布式数据集(resilient distributed datasets, RDDs)的分布式数据框架进行的。</p>
<h5 id="1-Spark核心组件"><a href="#1-Spark核心组件" class="headerlink" title="1.Spark核心组件"></a>1.Spark核心组件</h5><p>Spark 库本身包含很多应用元素，这些元素可以用到大部分大数据应用中，其中包括对大数据进行类似 SQL 查询的支持，机器学习和图算法，对实时流数据的支持。具体核心组件如下：</p>
<p><strong>Spark Core：</strong>包含 Spark 的基本功能；尤其是定义 RDD 的 API、操作以及这两者上的动作。其他 Spark 的库都是构建在 RDD 和 Spark Core 之上的。</p>
<p><strong>Spark SQL：</strong>提供通过 Apache Hive 的 SQL 变体 Hive 查询语言（HiveQL）与 Spark 进行交互的 API。每个数据库表被当做一个 RDD，Spark SQL 查询被转换为 Spark 操作。对熟悉 Hive 和 HiveQL 的人，Spar k可以拿来就用。</p>
<p><strong>Spark Streaming：</strong>允许对实时数据流进行处理和控制。很多实时数据库（如Apache Store）可以处理实时数据。Spark Streaming 允许程序能够像普通 RDD 一样处理实时数据。</p>
<p><strong>MLlib：</strong>一个常用机器学习算法库，算法被实现为对 RDD 的 Spark 操作。这个库包含可扩展的学习算法，比如分类、回归等需要对大量数据集进行迭代的操作。之前可选的大数据机器学习库 Mahout，将会转到 Spark，并在未来实现。</p>
<p><strong>GraphX：</strong>控制图、并行图操作和计算的一组算法和工具的集合。GraphX 扩展了 RDD API，包含控制图、创建子图、访问路径上所有顶点的操作。</p>
<p>由于这些组件满足了很多大数据需求，也满足了很多数据科学任务的算法和计算上的需要，Spark 快速流行起来。不仅如此，Spark 也提供了使用 Scala、Java 和Python 编写的 API；满足了不同团体的需求，允许更多数据科学家简便地采用 Spark 作为他们的大数据解决方案。</p>
<h5 id="2-Spark体系架构"><a href="#2-Spark体系架构" class="headerlink" title="2.Spark体系架构"></a>2.Spark体系架构</h5><p>Spark体系架构包括如下三个主要组件：</p>
<ul>
<li>数据存储</li>
<li>API</li>
<li>管理框架</li>
</ul>
<p><strong>数据存储：</strong>Spark 用 HDFS 文件系统存储数据。它可用于存储任何兼容于 Hadoop 的数据源，包括HDFS，Hbase，Cassandra等。</p>
<p><strong>API：</strong>利用 API，应用开发者可以用标准的 API 接口创建基于 Spark 的应用。Spark 提供 Scala，Java 和 Python 三种程序设计语言的 API。<a href="http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.package" target="_blank" rel="noopener">Scala API</a></p>
<p><strong>Spark基本概念：</strong></p>
<ul>
<li><code>Application：</code> 用户自己写的 Spark 应用程序，批处理作业的集合。Application 的 main 方法为应用程序的入口，用户通过 Spark 的 API，定义了 RDD 和对 RDD 的操作。</li>
<li><code>SparkContext：</code> Spark 最重要的 API，用户逻辑与 Spark 集群主要的交互接口，它会和 Cluster Master 交互，包括向它申请计算资源等。</li>
<li><code>Driver 和 Executor：</code>Spark 在执行每个 Application 的过程中会启动 Driver 和 Executor 两种 JVM 进程。Driver 进程为主控进程，负责执行用户 Application 中的 main 方法，提交 Job，并将 Job 转化为 Task，在各个 Executor 进程间协调 Task 的调度。运行在Worker上 的 Executor 进程负责执行 Task，并将结果返回给 Driver，同时为需要缓存的 RDD 提供存储功能。</li>
</ul>
<p><strong>资源管理：</strong></p>
<p>一组计算机的集合，每个计算机节点作为独立的计算资源，又可以虚拟出多个具备计算能力的虚拟机，这些虚拟机是集群中的计算单元。Spark 的核心模块专注于调度和管理虚拟机之上分布式计算任务的执行，集群中的计算资源则交给 Cluster Manager 这个角色来管理，Cluster Manager 可以为自带的Standalone、或第三方的 Yarn和 Mesos。</p>
<p>Cluster Manager 一般采用 Master-Slave 结构。以 Yarn 为例，部署 ResourceManager 服务的节点为 Master，负责集群中所有计算资源的统一管理和分配；部署 NodeManager 服务的节点为Slave，负责在当前节点创建一个或多个具备独立计算能力的 JVM 实例，在 Spark 中，这些节点也叫做 Worker。</p>
<p>另外还有一个 Client 节点的概念，是指用户提交Spark Application 时所在的节点。</p>
<p><strong>弹性分布式数据集(RDD)：</strong></p>
<p>弹性分布式数据集(RDD)是 Spark 框架中的核心概念。可以将 RDD 视作数据库中的一张表。其中可以保存任何类型的数据。Spark 将数据存储在不同分区上的 RDD 之中。</p>
<p>RDD 可以帮助重新安排计算并优化数据处理过程。</p>
<p>此外，它还具有容错性，因为RDD知道如何重新创建和重新计算数据集。</p>
<p>RDD 是不可变的。你可以用变换（Transformation）修改 RDD，但是这个变换所返回的是一个全新的RDD，而原有的 RDD 仍然保持不变。</p>
<p>RDD 支持两种类型的操作：</p>
<ul>
<li><code>变换（Transformation）</code>变换的返回值是一个新的 RDD 集合，而不是单个值。调用一个变换方法，不会有任何求值计算，它只获取一个 RDD 作为参数，然后返回一个新的 RDD。 变换函数包括：map，filter，flatMap，groupByKey，reduceByKey，aggregateByKey，pipe和coalesce。</li>
<li><code>行动（Action）</code>行动操作计算并返回一个新的值。当在一个 RDD 对象上调用行动函数时，会在这一时刻计算全部的数据处理查询并返回结果值。 行动操作包括：reduce，collect，count，first，take，countByKey 以及 foreach。</li>
</ul>
<h5 id="3-安装部署"><a href="#3-安装部署" class="headerlink" title="3.安装部署"></a>3.安装部署</h5><p>本文档是对 Spark 的一个快速入门。首先，我们通过 Spark 的交互式 shell 介绍一下 API（主要是 Scala），</p>
<p>更完整参考 programming guide：</p>
<h2 id="http-spark-apache-org-docs-latest-programming-guide-html"><a href="#http-spark-apache-org-docs-latest-programming-guide-html" class="headerlink" title="http://spark.apache.org/docs/latest/programming-guide.html"></a><a href="http://spark.apache.org/docs/latest/programming-guide.html" target="_blank" rel="noopener">http://spark.apache.org/docs/latest/programming-guide.html</a></h2><p><strong>(1). 安装前设置</strong></p>
<blockquote>
<blockquote>
<blockquote>
<p>注意：本小节部分，实验楼环境已经配置好，无需配置，可以跳过此步骤。</p>
</blockquote>
</blockquote>
</blockquote>
<p>在安装 Hadoop 之前，需要进入 Linux/Ubuntu 环境下，连接 Linux/Ubuntu 使用 SSH (安全 Shell)。按照仅给出简要步骤设立 Linux/Ubuntu 环境，对于本小节您可以百度。</p>
<ol>
<li>修改主机名</li>
<li>修改 IP</li>
<li>修改主机名和IP的映射关系</li>
<li>关闭防火墙</li>
<li>重启 Linux/Ubuntu</li>
<li>安装JDK</li>
</ol>
<p><strong>(2). 安装 Spark</strong></p>
<p>官网下载地址：</p>
<h2 id="http-spark-apache-org-downloads-html"><a href="#http-spark-apache-org-downloads-html" class="headerlink" title="http://spark.apache.org/downloads.html"></a><a href="http://spark.apache.org/downloads.html" target="_blank" rel="noopener">http://spark.apache.org/downloads.html</a></h2><p>本文档选择的是 <code>spark-1.6.1-bin-hadoop2.6</code> 版本，下载地址：</p>
<h2 id="https-d3kbcqa49mib13-cloudfront-net-spark-1-6-1-bin-hadoop2-6-tgz"><a href="#https-d3kbcqa49mib13-cloudfront-net-spark-1-6-1-bin-hadoop2-6-tgz" class="headerlink" title="https://d3kbcqa49mib13.cloudfront.net/spark-1.6.1-bin-hadoop2.6.tgz"></a><a href="https://d3kbcqa49mib13.cloudfront.net/spark-1.6.1-bin-hadoop2.6.tgz" target="_blank" rel="noopener">https://d3kbcqa49mib13.cloudfront.net/spark-1.6.1-bin-hadoop2.6.tgz</a></h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt; cd /home/shiyanlou</span><br><span class="line"></span><br><span class="line">&gt;&gt;sudo wget https://d3kbcqa49mib13.cloudfront.net/spark-1.6.1-bin-hadoop2.6.tgz</span><br><span class="line"></span><br><span class="line">&gt;&gt; sudo tar zxvf spark-1.6.1-bin-hadoop2.6.tgz</span><br></pre></td></tr></table></figure>
<p><strong>(3). 通过 Spark Shell 进行交互分析</strong></p>
<p>Spark shell 提供了简单的方式来学习 API，也提供了交互的方式来分析数据。Spark Shell 支持 Scala 和 Python，本文档选择使用 Scala 来进行介绍。</p>
<p><code>Scala:</code>是一门现代的多范式编程语言，以简练、优雅及类型安全的方式来表达常用编程模式。它平滑地集成了面向对象和函数语言的特性。Scala 运行于 Java 平台（JVM，Java 虚拟机），并兼容现有的 Java 程序。Scala 是 Spark 的主要编程语言，如果仅仅是写 Spark 应用，并非一定要用 Scala，用 Java、Python 都是可以的。使用 Scala 的优势是开发效率更高，代码更精简，并且可以通过 Spark Shell 进行交互式实时查询，方便排查问题。</p>
<p>1). <strong>启动 spark shell</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;cd spark-1.6.1-bin-hadoop2.6</span><br><span class="line"></span><br><span class="line">&gt;&gt;./bin/spark-shell.sh</span><br></pre></td></tr></table></figure>
<p>Spark 最主要的抽象概念是个分布式集合，也叫作弹性分布式数据集（Resilient Distributed Dataset – RDD）。它可被分发到集群各个节点上，进行并行操作。RDD 可以由 Hadoop InputFormats 读取 HDFS 文件创建得来，或者从其他 RDD 转换得到。下面我们就先利用 Spark 源代码目录下的 README 文件来新建一个 RDD：</p>
<p>我们从 <code>/home/shiyanlou/README.md</code> 文件新建一个 RDD，代码如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;`scala&gt; val textFile = sc.textFile(&quot;/home/shiyanlou/README.md&quot;)`</span><br><span class="line"></span><br><span class="line">&gt; textFile: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[1] at textFile at &lt;console&gt;:1</span><br></pre></td></tr></table></figure>
<p>下面我们就来演示 count() 和 first() 操作：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;  `scala&gt; textFile.count()`</span><br><span class="line">&gt;</span><br><span class="line">&gt; res0: Long = 95</span><br><span class="line">&gt;&gt;`scala&gt; textFile.first()`</span><br><span class="line">&gt;</span><br><span class="line">&gt;  res1: String = # Apache Spark</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure>
<p>接着演示 transformation，通过 filter transformation 来返回一个新的 RDD，代码如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;  `scala&gt; val linesWithSpark = textFile.filter(line =&gt; line.contains(&quot;Spark&quot;))` // 统计包含 Spark 的行数</span><br><span class="line"></span><br><span class="line">&gt; linesWithSpark: spark.RDD[String] = spark.FilteredRDD@7dd4af39</span><br><span class="line">&gt; </span><br><span class="line">&gt;&gt; `scala&gt; linesWithSpark.count()`   // 统计行数</span><br><span class="line"></span><br><span class="line">&gt; res2: Long = 17</span><br></pre></td></tr></table></figure>
<p>可以看到一共有 17 行内容包含 Spark，这与通过 Linux 命令 cat /home/shiyanlou/README.md | grep “Spark” -c 得到的结果一致，说明是正确的。action 和 transformation 可以用链式操作的方式结合使用，使代码更为简洁：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;`scala&gt; textFile.filter(line =&gt; line.contains(&quot;Spark&quot;)).count()`  // 统计包含 Spark 的行数</span><br><span class="line"></span><br><span class="line">&gt; res3: Long = 17</span><br></pre></td></tr></table></figure>
<p>RDD 的 actions 和 transformations 可用在更复杂的计算中，例如通过如下代码可以找到包含单词最多的那一行内容共有几个单词：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;`scala&gt; textFile.map(line =&gt; line.split(&quot; &quot;).size).reduce((a, b) =&gt; if (a &gt; b) a else b)`</span><br><span class="line"></span><br><span class="line">&gt; res4: Int = 14</span><br></pre></td></tr></table></figure>
<p>代码首先将每一行内容 map 为一个整数，这将创建一个新的 RDD，并在这个 RDD 中执行 reduce 操作，找到最大的数。map()、reduce() 中的参数是 Scala 的函数字面量（function literals，也称为闭包 closures），并且可以使用语言特征或 Scala/Java 的库。例如，通过使用 Math.max() 函数（需要导入 Java 的 Math 库），可以使上述代码更容易理解：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;`scala&gt; import java.lang.Math`</span><br><span class="line"></span><br><span class="line">&gt; import java.lang.Math</span><br><span class="line"></span><br><span class="line">&gt;&gt;`scala&gt; textFile.map(line =&gt; line.split(&quot; &quot;).size).reduce((a, b) =&gt; Math.max(a, b))`</span><br><span class="line">&gt;</span><br><span class="line">&gt; res5: Int = 16</span><br></pre></td></tr></table></figure>
<p>Hadoop 上的 MapReduce 是大家耳熟能详的一种通用数据流模式。在 Spark 中同样可以实现（下面这个例子也就是 WordCount）：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt; `scala&gt; val wordCounts = textFile.flatMap(line =&gt; line.split(&quot; &quot;)).map(word =&gt; (word, 1)).reduceByKey((a, b) =&gt; a + b)  `  // 实现单词统计</span><br><span class="line"></span><br><span class="line">&gt; wordCounts: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[4] at reduceByKey at &lt;console&gt;:2</span><br><span class="line"></span><br><span class="line">&gt;&gt; `scala&gt; wordCounts.collect()`   // 输出单词统计结果</span><br><span class="line"></span><br><span class="line">&gt; res7: Array[(String, Int)] = Array((package,1), (For,2), (Programs,1), (processing.,1), (Because,1), (The,1)...)</span><br></pre></td></tr></table></figure>
<p>2). <strong>RDD 缓存</strong></p>
<p>Spark 也支持在分布式的环境下基于内存的缓存，这样当数据需要重复使用的时候就很有帮助。比如当需要查找一个很小的 hot 数据集，或者运行一个类似 PageRank 的算法。</p>
<p>举个简单的例子，对 linesWithSpark RDD 数据集进行缓存，然后再调用 count() 会触发算子操作进行真正的计算，之后再次调用 count() 就不会再重复的计算，直接使用上一次计算的结果的 RDD 了：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;  `scala&gt; linesWithSpark.cache() `</span><br><span class="line"></span><br><span class="line">&gt; res8: spark.RDD[String] = spark.FilteredRDD@17e51082</span><br><span class="line"></span><br><span class="line">&gt;&gt; `scala&gt; linesWithSpark.count()`</span><br><span class="line"></span><br><span class="line">&gt; res9: Long = 17</span><br><span class="line"></span><br><span class="line">&gt;&gt; `scala&gt; linesWithSpark.count()`</span><br><span class="line"></span><br><span class="line">&gt; res10: Long = 17</span><br></pre></td></tr></table></figure>
<p>看起来缓存一个100行左右的文件很愚蠢，但是如果再非常大的数据集下就非常有用了，尤其是在成百上千的节点中传输 RDD 计算的结果。</p>
<p>3). <strong>Spark SQL 和 DataFrames</strong></p>
<p>Spark SQL 是 Spark 内嵌的模块，用于结构化数据。在 Spark 程序中可以使用 SQL 查询语句或 DataFrame API。DataFrames 和 SQL 提供了通用的方式来连接多种数据源，支持 Hive、Avro、Parquet、ORC、JSON、和 JDBC，并且可以在多种数据源之间执行 join 操作。</p>
<p>下面仍在 Spark shell 中演示一下 Spark SQL 的基本操作，该部分内容主要参考了 Spark SQL、DataFrames 和 Datasets 指南</p>
<h2 id="http-spark-apache-org-docs-latest-sql-programming-guide-htm"><a href="#http-spark-apache-org-docs-latest-sql-programming-guide-htm" class="headerlink" title="http://spark.apache.org/docs/latest/sql-programming-guide.htm"></a><a href="http://spark.apache.org/docs/latest/sql-programming-guide.htm" target="_blank" rel="noopener">http://spark.apache.org/docs/latest/sql-programming-guide.htm</a></h2><p>Spark SQL 的功能是通过 SQLContext 类来使用的，而创建 SQLContext 是通过 SparkContext 创建的。在 Spark shell 启动时，输出日志的最后有这么几条信息：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">17/05/25 15:16:34 INFO repl.SparkILoop: Created spark context..</span><br><span class="line"></span><br><span class="line">Spark context available as sc.</span><br><span class="line"></span><br><span class="line">17/05/25 15:16:34 INFO repl.SparkILoop: Created sql context..</span><br><span class="line"></span><br><span class="line">SQL context available as sqlContext.</span><br></pre></td></tr></table></figure>
<p>这些信息表明 SparkContent 和 SQLContext 都已经初始化好了，可通过对应的 sc、sqlContext 变量直接进行访问。</p>
<p>使用 SQLContext 可以从现有的 RDD 或数据源创建 DataFrames。作为示例，我们通过 Spark 提供的 JSON 格式的数据源文件 spark-1.6.1-bin-hadoop2.6/examples/src/main/resources/people.json 来进行演示，该数据源内容如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&#123;&quot;name&quot;:&quot;Michael&quot;&#125;</span><br><span class="line"></span><br><span class="line">&#123;&quot;name&quot;:&quot;Andy&quot;, &quot;age&quot;:30&#125;</span><br><span class="line"></span><br><span class="line">&#123;&quot;name&quot;:&quot;Justin&quot;, &quot;age&quot;:19&#125;</span><br></pre></td></tr></table></figure>
<p>执行如下命令导入数据源，并输出内容：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;`scala&gt; val df = sqlContext.read.json(&quot;home/shiyanlou/spark-1.6.1-bin-hadoop2.6/examples/src/main/resources/people.json&quot;)`</span><br><span class="line"></span><br><span class="line">&gt; df: org.apache.spark.sql.DataFrame = [age: bigint, name: string]</span><br><span class="line"></span><br><span class="line">&gt;&gt; `scala&gt; df.show()`  // 输出数据源内容</span><br><span class="line"></span><br><span class="line">&gt;     +----+-------+   </span><br><span class="line">    | age|   name|</span><br><span class="line">    +----+-------+</span><br><span class="line">    |null|Michael|</span><br><span class="line">    |  30|   Andy|</span><br><span class="line">    |  19| Justin|</span><br><span class="line">    +----+-------+</span><br></pre></td></tr></table></figure>
<p>接着，我们来演示 DataFrames 处理结构化数据的一些基本操作：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt; `scala&gt; df.select(&quot;name&quot;).show() `  // 只显示 &quot;name&quot; 列</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">&gt;     +-------+   </span><br><span class="line">    |   name|</span><br><span class="line">    +-------+</span><br><span class="line">    |Michael|</span><br><span class="line">    |   Andy|</span><br><span class="line">    | Justin|</span><br><span class="line">    +-------+</span><br><span class="line"></span><br><span class="line">&gt;&gt; `scala&gt; df.select(df(&quot;name&quot;), df(&quot;age&quot;) + 1).show()   ` // 将 &quot;age&quot; 加 1</span><br><span class="line"></span><br><span class="line">&gt;     +-------+---------+</span><br><span class="line">    |   name|(age + 1)|</span><br><span class="line">    +-------+---------+</span><br><span class="line">    |Michael|     null|</span><br><span class="line">    |   Andy|       31|</span><br><span class="line">    | Justin|       20|</span><br><span class="line">    +-------+---------+</span><br><span class="line"></span><br><span class="line">&gt;&gt; `scala&gt; df.filter(df(&quot;age&quot;) &gt; 21).show()  `  //条件语句</span><br><span class="line"></span><br><span class="line">&gt;     +---+----+</span><br><span class="line">    |age|name|</span><br><span class="line">    +---+----+</span><br><span class="line">    | 30|Andy|</span><br><span class="line">    +---+----+</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">&gt;&gt; `scala&gt; df.groupBy(&quot;age&quot;).count().show()   ` // groupBy 操作</span><br><span class="line"></span><br><span class="line">&gt;      +----+-----+</span><br><span class="line">     | age|count|</span><br><span class="line">     +----+-----+</span><br><span class="line">     |null|    1|</span><br><span class="line">     |  19|    1|</span><br><span class="line">     |  30|    1|</span><br><span class="line">     +----+-----+</span><br></pre></td></tr></table></figure>
<p>当然，我们也可以使用 SQL 语句来进行操作：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;`scala&gt; .registerTempTable(&quot;people&quot;)  `    // 将 DataFrame 注册为临时表 people</span><br><span class="line">&gt;&gt; `scala&gt; val result = sqlContext.sql(&quot;SELECT name, age FROM people WHERE age &gt;= 13 AND age &lt;= 19&quot;)` // 执行 SQL 查询</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">&gt;&gt; `scala&gt; result.show()  ` //输出结果</span><br><span class="line"></span><br><span class="line">&gt;     +----+-------+   </span><br><span class="line">    | name |  age|</span><br><span class="line">    +----+-------+</span><br><span class="line">    |Justin|   19|</span><br><span class="line">    +----+-------+</span><br></pre></td></tr></table></figure>
<p>更多的功能可以查看完整的 DataFrames API:</p>
<h2 id="http-spark-apache-org-docs-latest-api-scala-index-html-org-apache-spark-sql-DataFrame"><a href="#http-spark-apache-org-docs-latest-api-scala-index-html-org-apache-spark-sql-DataFrame" class="headerlink" title="http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.DataFrame"></a><a href="http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.DataFrame" target="_blank" rel="noopener">http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.DataFrame</a></h2><p>此外 DataFrames 也包含了丰富的 DataFrames Function:</p>
<h2 id="http-spark-apache-org-docs-latest-api-scala-index-html-org-apache-spark-sql-functions"><a href="#http-spark-apache-org-docs-latest-api-scala-index-html-org-apache-spark-sql-functions" class="headerlink" title="http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.functions$"></a><a href="http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.functions" target="_blank" rel="noopener">http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.functions$</a></h2><h2 id="5-总结"><a href="#5-总结" class="headerlink" title="5. 总结"></a>5. 总结</h2><p>Apache Spark 是一个新兴的大数据处理通用引擎，提供了分布式的内存抽象。Spark 拥有多种语言的函数式编程 API，提供了除 map 和 reduce 之外更多的运算符，这些操作是通过一个称作弹性分布式数据集(resilient distributed datasets, RDDs)的分布式数据框架进行的。这篇文档的目的是帮助你快速入门，完成单机上的 Spark 安装与使用，本文档是 Spark 快速入门，主要介绍了 Spark shell 、RDD、Spark SQL 等，希望对您有所帮助。</p>
<p>更多学习资料请参考：</p>
<p>Spark 集群部署：</p>
<h2 id="http-spark-apache-org-docs-latest-cluster-overview-html"><a href="#http-spark-apache-org-docs-latest-cluster-overview-html" class="headerlink" title="http://spark.apache.org/docs/latest/cluster-overview.html"></a><a href="http://spark.apache.org/docs/latest/cluster-overview.html" target="_blank" rel="noopener">http://spark.apache.org/docs/latest/cluster-overview.html</a></h2><p>Spark 编程指南：</p>
<h2 id="http-spark-apache-org-docs-latest-programming-guide-html-1"><a href="#http-spark-apache-org-docs-latest-programming-guide-html-1" class="headerlink" title="http://spark.apache.org/docs/latest/programming-guide.html"></a><a href="http://spark.apache.org/docs/latest/programming-guide.html" target="_blank" rel="noopener">http://spark.apache.org/docs/latest/programming-guide.html</a></h2><p>Spark SQL、DataFrames 和 Datasets 指南：</p>
<h2 id="http-spark-apache-org-docs-latest-sql-programming-guide-html"><a href="#http-spark-apache-org-docs-latest-sql-programming-guide-html" class="headerlink" title="http://spark.apache.org/docs/latest/sql-programming-guide.html"></a><a href="http://spark.apache.org/docs/latest/sql-programming-guide.html" target="_blank" rel="noopener">http://spark.apache.org/docs/latest/sql-programming-guide.html</a></h2>
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/大数据处理/" rel="tag"># 大数据处理</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2019/03/23/CNN-LeNet5/" rel="next" title="CNN---LeNet5">
                <i class="fa fa-chevron-left"></i> CNN---LeNet5
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            Table of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            Overview
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">joyes</p>
              <p class="site-description motion-element" itemprop="description">YYT</p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">5</span>
                  <span class="site-state-item-name">posts</span>
                </a>
              </div>
            

            

            
              
              
              <div class="site-state-item site-state-tags">
                
                  <span class="site-state-item-count">4</span>
                  <span class="site-state-item-name">tags</span>
                
              </div>
            

          </nav>

          

          

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-4"><a class="nav-link" href="#Spark简介"><span class="nav-number">1.</span> <span class="nav-text">Spark简介</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Spark生态圈"><span class="nav-number">2.</span> <span class="nav-text">Spark生态圈</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Spark网上推荐学习路线"><span class="nav-number">3.</span> <span class="nav-text">Spark网上推荐学习路线</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#实验楼Spark快速入门教程-参考链接"><span class="nav-number">4.</span> <span class="nav-text">实验楼Spark快速入门教程   参考链接</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#1-Spark核心组件"><span class="nav-number">4.1.</span> <span class="nav-text">1.Spark核心组件</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#2-Spark体系架构"><span class="nav-number">4.2.</span> <span class="nav-text">2.Spark体系架构</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#3-安装部署"><span class="nav-number">4.3.</span> <span class="nav-text">3.安装部署</span></a></li></ol></li></ol><li class="nav-item nav-level-2"><a class="nav-link" href="#http-spark-apache-org-docs-latest-programming-guide-html"><span class="nav-number"></span> <span class="nav-text">http://spark.apache.org/docs/latest/programming-guide.html</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#http-spark-apache-org-downloads-html"><span class="nav-number"></span> <span class="nav-text">http://spark.apache.org/downloads.html</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#https-d3kbcqa49mib13-cloudfront-net-spark-1-6-1-bin-hadoop2-6-tgz"><span class="nav-number"></span> <span class="nav-text">https://d3kbcqa49mib13.cloudfront.net/spark-1.6.1-bin-hadoop2.6.tgz</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#http-spark-apache-org-docs-latest-sql-programming-guide-htm"><span class="nav-number"></span> <span class="nav-text">http://spark.apache.org/docs/latest/sql-programming-guide.htm</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#http-spark-apache-org-docs-latest-api-scala-index-html-org-apache-spark-sql-DataFrame"><span class="nav-number"></span> <span class="nav-text">http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.DataFrame</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#http-spark-apache-org-docs-latest-api-scala-index-html-org-apache-spark-sql-functions"><span class="nav-number"></span> <span class="nav-text">http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.functions$</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#5-总结"><span class="nav-number"></span> <span class="nav-text">5. 总结</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#http-spark-apache-org-docs-latest-cluster-overview-html"><span class="nav-number"></span> <span class="nav-text">http://spark.apache.org/docs/latest/cluster-overview.html</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#http-spark-apache-org-docs-latest-programming-guide-html-1"><span class="nav-number"></span> <span class="nav-text">http://spark.apache.org/docs/latest/programming-guide.html</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#http-spark-apache-org-docs-latest-sql-programming-guide-html"><span class="nav-number"></span> <span class="nav-text">http://spark.apache.org/docs/latest/sql-programming-guide.html</span></a></li></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">joyes</span>

  
</div>


  <div class="powered-by">Powered by <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a></div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">Theme &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Muse</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  





  

  

  

  
  

  

  

  

</body>
</html>
